{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b1086b-b3b3-4602-8efa-d8e2713e9d8c",
   "metadata": {},
   "source": [
    "Test check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21aa517-3c27-4ed8-a478-3ec9cc3a5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/nasty280700/spark-3.2.4-bin-hadoop3.2')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5226cd4c-532f-4375-8783-57fe73dc7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/nasty280700/.local/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nasty280700/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/nasty280700/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/nasty280700/.local/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/nasty280700/.local/lib/python3.10/site-packages (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/nasty280700/.local/lib/python3.10/site-packages (from pyarrow) (1.25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastparquet in /home/nasty280700/.local/lib/python3.10/site-packages (2023.7.0)\n",
      "Requirement already satisfied: fsspec in /home/nasty280700/.local/lib/python3.10/site-packages (from fastparquet) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /home/nasty280700/.local/lib/python3.10/site-packages (from fastparquet) (23.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/nasty280700/.local/lib/python3.10/site-packages (from fastparquet) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/nasty280700/.local/lib/python3.10/site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/nasty280700/.local/lib/python3.10/site-packages (from fastparquet) (1.25.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nasty280700/.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/nasty280700/.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea6c629-2859-48f4-84f1-f094c7b38cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<---***--- START ---***--->>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 12:26:40 WARN Utils: Your hostname, nasty280700-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/08/27 12:26:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nasty280700/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/27 12:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(2, 'Alice', 'Black', 'Washington')\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----------+\n",
      "|customer_id|fname|  lname|     state|\n",
      "+-----------+-----+-------+----------+\n",
      "|          1|  Tom|  Green|Washington|\n",
      "|          2|Alice|  Black|Washington|\n",
      "|          3|Jimmy| Kimmel|     Texas|\n",
      "|          4|  Bob|  White|     Texas|\n",
      "|          5|Kenny|   West|   Atlanta|\n",
      "|          6| Kate|Tompson|     Texas|\n",
      "+-----------+-----+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    states|\n",
      "+----------+\n",
      "|     Texas|\n",
      "|Washington|\n",
      "|   Atlanta|\n",
      "+----------+\n",
      "\n",
      "<<---***--- END ---***--->>\n"
     ]
    }
   ],
   "source": [
    "print(\"<<---***--- START ---***--->>\")\n",
    "\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName('pyspark_example')\n",
    " .enableHiveSupport()\n",
    " .getOrCreate())\n",
    " \n",
    "rows = [\n",
    "(1,\"Tom\", \"Green\", \"Washington\"),(2,\"Alice\", \"Black\", \"Washington\"),\n",
    "(3,\"Jimmy\", \"Kimmel\", \"Texas\"), (4,\"Bob\", \"White\", \"Texas\"),\n",
    "(5,\"Kenny\", \"West\", \"Atlanta\"), (6,\"Kate\", \"Tompson\", \"Texas\")\n",
    "]\n",
    "\n",
    "print(type(rows))\n",
    "print(rows[1])\n",
    "print(type(rows[1]))\n",
    "\n",
    "schema = \"customer_id BIGINT, fname STRING, lname STRING, state STRING\"\n",
    "customersDF = spark.createDataFrame(rows, schema)\n",
    "customersDF.show()\n",
    "\n",
    "customersDF.createOrReplaceTempView(\"tmp_clients\")\n",
    "\n",
    "clientsDF = spark.sql(\"\"\"\n",
    "select distinct state as states from tmp_clients\n",
    "\"\"\")\n",
    "\n",
    "clientsDF.show()\n",
    "\n",
    "print(\"<<---***--- END ---***--->>\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "944a7cf5-eeac-43f2-9c8e-65684812efae",
   "metadata": {},
   "source": [
    "Project assignment 2\n",
    "task 2.1\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cf0f57-5713-4598-a6ef-5385d719f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+\n",
      "|row_id|         discipline|season|\n",
      "+------+-------------------+------+\n",
      "|     1|          athletics|summer|\n",
      "|     2|           swimming|summer|\n",
      "|     3|artistic gymnastics|summer|\n",
      "|     4|       cycling road|summer|\n",
      "|     5|         volleyball|summer|\n",
      "|     6|          bobsleigh|winter|\n",
      "|     7|         ice hockey|winter|\n",
      "|     8|           biathlon|winter|\n",
      "|     9|        ice skating|winter|\n",
      "|    10|       snowboarding|winter|\n",
      "+------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Task1\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "data = [\n",
    "    Row(row_id=1, discipline=\"athletics\", season=\"summer\"),\n",
    "    Row(row_id=2, discipline=\"swimming\", season=\"summer\"),\n",
    "    Row(row_id=3, discipline=\"artistic gymnastics\", season=\"summer\"),\n",
    "    Row(row_id=4, discipline=\"cycling road\", season=\"summer\"),\n",
    "    Row(row_id=5, discipline=\"volleyball\", season=\"summer\"),\n",
    "    Row(row_id=6, discipline=\"bobsleigh\", season=\"winter\"),\n",
    "    Row(row_id=7, discipline=\"ice hockey\", season=\"winter\"),\n",
    "    Row(row_id=8, discipline=\"biathlon\", season=\"winter\"),\n",
    "    Row(row_id=9, discipline=\"ice skating\", season=\"winter\"),\n",
    "    Row(row_id=10, discipline=\"snowboarding\", season=\"winter\")\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "df.coalesce(1).write.option(\"sep\", \"\\t\").csv(\"task1_olymp_folder.csv\", header=True, mode=\"overwrite\")\n",
    "df.toPandas().to_csv(\"task1_olymp.csv\", index = False, sep=\"\\t\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a87c6feb-a94f-47a6-a9c8-7a3051b0d178",
   "metadata": {},
   "source": [
    "Project assignment 2\n",
    "task 2.1\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481ab412-8b4d-4ddf-8358-97c3eb1f9e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                Name|                 NOC|         Discipline|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|     AALERUD Katrine|              Norway|       Cycling Road|\n",
      "|         ABAD Nestor|               Spain|Artistic Gymnastics|\n",
      "|   ABAGNALE Giovanni|               Italy|             Rowing|\n",
      "|      ABALDE Alberto|               Spain|         Basketball|\n",
      "|       ABALDE Tamara|               Spain|         Basketball|\n",
      "|           ABALO Luc|              France|           Handball|\n",
      "|        ABAROA Cesar|               Chile|             Rowing|\n",
      "|       ABASS Abobakr|               Sudan|           Swimming|\n",
      "|    ABBASALI Hamideh|Islamic Republic ...|             Karate|\n",
      "|       ABBASOV Islam|          Azerbaijan|          Wrestling|\n",
      "|        ABBINGH Lois|         Netherlands|           Handball|\n",
      "|         ABBOT Emily|           Australia|Rhythmic Gymnastics|\n",
      "|       ABBOTT Monica|United States of ...|  Baseball/Softball|\n",
      "|ABDALLA Abubaker ...|               Qatar|          Athletics|\n",
      "|      ABDALLA Maryam|               Egypt|  Artistic Swimming|\n",
      "|      ABDALLAH Shahd|               Egypt|  Artistic Swimming|\n",
      "| ABDALRASOOL Mohamed|               Sudan|               Judo|\n",
      "|   ABDEL LATIF Radwa|               Egypt|           Shooting|\n",
      "|    ABDEL RAZEK Samy|               Egypt|           Shooting|\n",
      "|   ABDELAZIZ Abdalla|               Egypt|             Karate|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------+\n",
      "|          Discipline|athletes_count|\n",
      "+--------------------+--------------+\n",
      "|              Tennis|           178|\n",
      "|              Boxing|           270|\n",
      "|   Marathon Swimming|            49|\n",
      "|                Golf|           115|\n",
      "|              Rowing|           496|\n",
      "|   Baseball/Softball|           220|\n",
      "|                Judo|           373|\n",
      "|             Sailing|           336|\n",
      "|            Swimming|           743|\n",
      "|Cycling BMX Frees...|            19|\n",
      "|          Basketball|           280|\n",
      "|            Handball|           343|\n",
      "| Rhythmic Gymnastics|            95|\n",
      "|              Karate|            77|\n",
      "|           Triathlon|           106|\n",
      "|           Badminton|           164|\n",
      "|        Canoe Sprint|           236|\n",
      "|           Athletics|          2068|\n",
      "|       Cycling Track|           208|\n",
      "|    Beach Volleyball|            90|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Task2\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \";\").csv(\"Athletes.csv\", header=True)\n",
    "\n",
    "df.createOrReplaceTempView(\"athletes\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_count = spark.sql(\"\"\"\n",
    "select Discipline, \n",
    "    count(Name) as athletes_count \n",
    "from athletes \n",
    "group by Discipline\n",
    "\"\"\")\n",
    "df_count.show()\n",
    "\n",
    "df_count.write.parquet(\"discipline_count_folder.parquet\", mode=\"overwrite\")\n",
    "df_count.toPandas().to_parquet(\"discipline_count.parquet\", index = False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff17784b-7e8f-48f3-929d-2168538defb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discipline</th>\n",
       "      <th>athletes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tennis</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boxing</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marathon Swimming</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golf</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rowing</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Discipline  athletes_count\n",
       "0             Tennis             178\n",
       "1             Boxing             270\n",
       "2  Marathon Swimming              49\n",
       "3               Golf             115\n",
       "4             Rowing             496"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table(\"discipline_count.parquet\")\n",
    "#table = pq.read_table(\"/home/nasty280700/spark-3.2.4-bin-hadoop3.2/discipline_count.parquet\")\n",
    "\n",
    "df = table.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b73a5bf5-37b6-45a6-96af-634d7776c997",
   "metadata": {},
   "source": [
    "Project assignment 2\n",
    "task 2.1\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a34abe-7668-4456-afe4-e89d1749835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          Discipline|athletes_count|\n",
      "+--------------------+--------------+\n",
      "|              Tennis|           178|\n",
      "|              Boxing|           270|\n",
      "|   Marathon Swimming|            49|\n",
      "|                Golf|           115|\n",
      "|              Rowing|           496|\n",
      "|   Baseball/Softball|           220|\n",
      "|                Judo|           373|\n",
      "|             Sailing|           336|\n",
      "|            Swimming|           743|\n",
      "|Cycling BMX Frees...|            19|\n",
      "|          Basketball|           280|\n",
      "|            Handball|           343|\n",
      "| Rhythmic Gymnastics|            95|\n",
      "|              Karate|            77|\n",
      "|           Triathlon|           106|\n",
      "|           Badminton|           164|\n",
      "|        Canoe Sprint|           236|\n",
      "|           Athletics|          2068|\n",
      "|       Cycling Track|           208|\n",
      "|    Beach Volleyball|            90|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+------+------+\n",
      "|         Discipline|athletes_count|row_id|season|\n",
      "+-------------------+--------------+------+------+\n",
      "|          Athletics|          2068|     1|summer|\n",
      "|           Swimming|           743|     2|summer|\n",
      "|Artistic Gymnastics|           187|     3|summer|\n",
      "|       Cycling Road|           190|     4|summer|\n",
      "|         Volleyball|           274|     5|summer|\n",
      "+-------------------+--------------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Task3\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "df_file = spark.read.option(\"delimiter\", \";\").csv(\"Athletes.csv\", header=True)\n",
    "\n",
    "df_count = df_file.groupBy(\"Discipline\").agg(count(\"Name\").alias(\"athletes_count\"))\n",
    "df_count.show()\n",
    "\n",
    "data = [\n",
    "    Row(row_id=1, discipline=\"athletics\", season=\"summer\"),\n",
    "    Row(row_id=2, discipline=\"swimming\", season=\"summer\"),\n",
    "    Row(row_id=3, discipline=\"artistic gymnastics\", season=\"summer\"),\n",
    "    Row(row_id=4, discipline=\"cycling road\", season=\"summer\"),\n",
    "    Row(row_id=5, discipline=\"volleyball\", season=\"summer\"),\n",
    "    Row(row_id=6, discipline=\"bobsleigh\", season=\"winter\"),\n",
    "    Row(row_id=7, discipline=\"ice hockey\", season=\"winter\"),\n",
    "    Row(row_id=8, discipline=\"biathlon\", season=\"winter\"),\n",
    "    Row(row_id=9, discipline=\"ice skating\", season=\"winter\"),\n",
    "    Row(row_id=10, discipline=\"snowboarding\", season=\"winter\")\n",
    "]\n",
    "\n",
    "df_task1 = spark.createDataFrame(data)\n",
    "df_task1 = df_task1.withColumnRenamed(\"discipline\", \"Discipline\")\n",
    "df_task1 = df_task1.withColumn(\"Discipline\", initcap(df_task1[\"Discipline\"]))\n",
    "\n",
    "\n",
    "res_df = df_count.join(df_task1, \"Discipline\", 'inner')\n",
    "res_df.show()\n",
    "\n",
    "\n",
    "res_df.write.parquet(\"1_join_2_folder.parquet\", mode=\"overwrite\")\n",
    "res_df.toPandas().to_parquet(\"1_join_2.parquet\", index = False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c67d7d-a596-40ca-938a-a18096f2ad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discipline</th>\n",
       "      <th>athletes_count</th>\n",
       "      <th>row_id</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athletics</td>\n",
       "      <td>2068</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swimming</td>\n",
       "      <td>743</td>\n",
       "      <td>2</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artistic Gymnastics</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cycling Road</td>\n",
       "      <td>190</td>\n",
       "      <td>4</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Volleyball</td>\n",
       "      <td>274</td>\n",
       "      <td>5</td>\n",
       "      <td>summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Discipline  athletes_count  row_id  season\n",
       "0            Athletics            2068       1  summer\n",
       "1             Swimming             743       2  summer\n",
       "2  Artistic Gymnastics             187       3  summer\n",
       "3         Cycling Road             190       4  summer\n",
       "4           Volleyball             274       5  summer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#table = pq.read_table(\"1_join_2.parquet\")\n",
    "table = pq.read_table(\"/home/nasty280700/spark-3.2.4-bin-hadoop3.2/1_join_2.parquet\")\n",
    "\n",
    "df = table.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637f3a3-4f4d-4d4a-9aca-4b41d2aef91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3d3ea-29b8-4ed2-b014-3663a79f8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e311c8e-8cc6-4a16-90bd-82f518d5a322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58681ba-644c-4e67-a2f7-e982798b02d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
